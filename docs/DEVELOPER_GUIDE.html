<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Brain Assistant: Developer Guide</title>
    <style>
        :root {
            --primary: #c0392b;
            --text: #2c3e50;
            --bg: #f9fbfd;
            --code-bg: #f5f7f9;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: var(--text);
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
            background: var(--bg);
        }
        h1, h2, h3 { color: #1a2a3a; margin-top: 2rem; }
        h1 { font-size: 2.5rem; border-bottom: 2px solid var(--primary); padding-bottom: 0.5rem; }
        .hero { background: white; padding: 2rem; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); margin-bottom: 2rem; }
        code { background: var(--code-bg); padding: 0.2rem 0.4rem; border-radius: 4px; font-family: "Menlo", "Monaco", monospace; font-size: 0.9em; }
        pre { background: var(--code-bg); padding: 1rem; border-radius: 8px; overflow-x: auto; }
        .callout { background: #fee; border-left: 4px solid var(--primary); padding: 1rem; margin: 1rem 0; border-radius: 0 8px 8px 0; }
        .img-container { text-align: center; margin: 2rem 0; background: white; padding: 1rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
        img { max-width: 100%; height: auto; }
        .badge { display: inline-block; padding: 0.25rem 0.5rem; border-radius: 12px; font-size: 0.8rem; font-weight: bold; background: #eee; }
    </style>
</head>
<body>

    <header class="hero">
        <h1>Brain Assistant Developer Guide</h1>
        <p><strong>Building Private Intelligence for the NUC Cluster</strong></p>
        <p>This guide explains the architecture, development workflow, and testing strategies for the distributed AI mesh.</p>
    </header>

    <h2>1. System Architecture</h2>
    
    <h3>The "Mesh" (Physical & Logical)</h3>
    <p>The system is distributed across four physical nodes in a fault-tolerant mesh architecture:</p>
    
    <h3>üñ•Ô∏è NUC-1: Orchestrator & Knowledge Hub</h3>
    <p><strong>Role:</strong> Central orchestration, semantic search, secret management.</p>
    <ul>
        <li><strong>ChromaDB (Vector Database):</strong> Semantic search index over your brain notes. All `/brain_search` queries hit this service.</li>
        <li><strong>Vaultwarden (Secrets Manager):</strong> Encrypted storage for API keys, tokens, and credentials. Never stored in plaintext.</li>
        <li><strong>Syncthing (Introducer):</strong> Coordinates file sync between NUC-2, NUC-3, and NUC-1.</li>
        <li><strong>Docker Stack:</strong> Runs Khoj (or replacement) service for indexed content.</li>
        <li><strong>Key Endpoint:</strong> <code>http://nuc-1.local:42110/api/search</code> for brain content retrieval.</li>
    </ul>
    <p><strong>For Developers:</strong> When working with `/brain_search`, understand that NUC-1 is the latency bottleneck. ChromaDB query performance directly affects user experience.</p>

    <h3>üñ•Ô∏è NUC-2: Agent Host (Slack Bot Service)</h3>
    <p><strong>Role:</strong> The active brain‚Äîruns the Slack bot and coordinates all tool execution.</p>
    <ul>
        <li><strong>SlackAgent Service:</strong> The main Python process running in `systemd` as <code>brain-slack-bot</code>.</li>
        <li><strong>Python Environment:</strong> Virtual environment at <code>~/agents/venv</code>.</li>
        <li><strong>Tool Execution Engine:</strong> All tool invocations (brain_search, web_search, facts) run here.</li>
        <li><strong>Syncthing Sync:</strong> Keeps <code>/home/earchibald/brain</code> in sync with NUC-1 and NUC-3.</li>
        <li><strong>Logs:</strong> <code>journalctl -u brain-slack-bot -f</code> for live debugging.</li>
    </ul>
    <p><strong>For Developers:</strong> NUC-2 is where you deploy your changes. All testing should happen here before production rollout.</p>

    <h3>üñ•Ô∏è NUC-3: Storage Hub & Backup</h3>
    <p><strong>Role:</strong> Persistent file storage, backups, and off-site replication.</p>
    <ul>
        <li><strong>NAS Storage:</strong> Syncthing target for brain folder backups.</li>
        <li><strong>Restic Backups:</strong> Local encrypted backups to external drive and pCloud via rclone.</li>
        <li><strong>Docker Syncthing:</strong> Runs Syncthing in a container with stable configuration.</li>
        <li><strong>Backup Scripts:</strong> Automated daily backup via <code>backup_brain.sh</code> (3 AM) and <code>sync_to_pcloud.sh</code> (3:30 AM).</li>
        <li><strong>Cron Jobs:</strong> Monitoring and health checks for backup integrity.</li>
    </ul>
    <p><strong>For Developers:</strong> NUC-3 failure affects backups, not live operations. NUC-1 and NUC-2 can run independently.</p>

    <h3>üíª Mac Mini: Primary Inference Engine</h3>
    <p><strong>Role:</strong> Primary local LLM inference for privacy and speed.</p>
    <ul>
        <li><strong>Ollama Service:</strong> Runs LLM models locally at <code>http://m1-mini.local:11434</code>.</li>
        <li><strong>Models Loaded:</strong>
            <ul>
                <li><code>llama3.2</code> (Active) - Fast, 8B parameters, good context understanding.</li>
                <li><code>nomic-embed-text</code> (Embedding) - For semantic search vector generation.</li>
            </ul>
        </li>
        <li><strong>100% Private:</strong> All inference stays on your network. No external API calls for Ollama mode.</li>
        <li><strong>Environment:</strong> <code>OLLAMA_HOST=0.0.0.0</code> enables network access from NUCs.</li>
    </ul>
    <p><strong>For Developers:</strong> If inference is slow, check Mac Mini CPU load. Ollama will queue requests if overloaded.</p>

    <h3>üíª M4 Pro Laptop: Auxiliary Inference (High-Speed)</h3>
    <p><strong>Role:</strong> Auxiliary LLM inference provider for faster responses when needed.</p>
    <ul>
        <li><strong>Hardware:</strong> M4 Pro chip with superior performance for quick inference.</li>
        <li><strong>Ollama Service:</strong> Runs at <code>http://[m4-laptop-hostname]:11434</code> (when connected).</li>
        <li><strong>Models Loaded:</strong>
            <ul>
                <li><code>llama3.1:8b</code> (Active) - Faster inference on M4 Pro hardware.</li>
            </ul>
        </li>
        <li><strong>Use Case:</strong> When latency is critical or Mac Mini is under heavy load.</li>
        <li><strong>Availability:</strong> Mobile/intermittent (only when laptop is on network and Ollama running).</li>
    </ul>
    <p><strong>For Developers:</strong> Configure the bot to fall back to Mac Mini if M4 Pro is unavailable. Consider load balancing between both providers.</p>

    <h3>The Request Flow (How Every Message Gets Processed)</h3>
    <p>Understanding the complete request lifecycle is critical for debugging and extending the agent.</p>
    <div class="img-container">
        <img src="report_images/tool_loop_sequence.png" alt="Tool Loop Sequence Diagram">
    </div>

    <ol>
        <li><strong>Intent Classification:</strong> Pre-process hook (<code>hooks/intent_classifier.py</code>) analyzes the message to classify intent (RESEARCH, KNOWLEDGE, PERSONAL, etc.).</li>
        <li><strong>LLM Prompt Generation:</strong> SlackAgent constructs the prompt with tools available based on intent classification.</li>
        <li><strong>Tool Execution:</strong> ToolExecutor (<code>tools/tool_executor.py</code>) parses LLM output and executes selected tools safely with timeouts.</li>
        <li><strong>Source Tracking:</strong> Each tool records its sources. SourceTracker (<code>hooks/source_tracker.py</code>) maintains context-local tracking.</li>
        <li><strong>Citation Generation:</strong> Post-process hook (<code>hooks/citation_hook.py</code>) formats citations and appends them to the response.</li>
    </ol>

    <p><strong>Key Implementation Files:</strong></p>
    <ul>
        <li><code>agents/slack_agent.py</code> - Main orchestrator. Entry point for all logic.</li>
        <li><code>slack_bot/hooks/</code> - Pre and post-processing hooks for extensibility.</li>
        <li><code>slack_bot/tools/tool_executor.py</code> - Safe, isolated execution environment with 15-second timeout.</li>
        <li><code>slack_bot/tools/builtin/</code> - Built-in tools: brain_search, web_search, facts.</li>
    </ul>

    <h2>2. Hardware & Network Prerequisites</h2>
    
    <h3>Network Topology</h3>
    <ul>
        <li>All NUCs, Mac Mini, and M4 Pro (when connected) are on the same local network (192.168.x.x).</li>
        <li>Service discovery via mDNS: <code>nuc-1.local</code>, <code>nuc-2.local</code>, <code>nuc-3.local</code>, <code>m1-mini.local</code>, and M4 Pro hostname.</li>
        <li>SSH passwordless access configured for <code>earchibald@</code> user on all hosts.</li>
        <li>Syncthing mesh keeps all three NUCs in continuous sync.</li>
        <li>M4 Pro provides auxiliary inference when available on network.</li>
    </ul>

    <h3>Service Dependencies</h3>
    <p>Before extending the system, understand the dependency chain:</p>
    <pre><code>SlackAgent (NUC-2)
    ‚îú‚îÄ Ollama (Mac Mini) - Primary inference
    ‚îú‚îÄ Ollama (M4 Pro) - Auxiliary inference (optional)
    ‚îú‚îÄ ChromaDB (NUC-1) - For semantic search
    ‚îú‚îÄ Vaultwarden (NUC-1) - For secrets
    ‚îî‚îÄ Syncthing (All) - For file sync</code></pre>
    <p>If any dependency is down, the bot degrades gracefully (falls back to chat-only mode). M4 Pro is optional and improves performance when available.</p>

    <h2>3. Development Workflow ("The Flow")</h2>
    <div class="callout">
        <strong>Core Philosophy:</strong> Test First. Deploy Safer.
    </div>

    <h3>Environment Setup</h3>
    <pre><code># Assume minimal dependencies initially
pyenv shell 3.12.12
python -m venv venv
source venv/bin/activate
pip install -r requirements-dev.txt
</code></pre>

    <h3>The TDD Loop</h3>
    <ol>
        <li><strong>Red:</strong> Write a failing test in <code>tests/red/</code>. Use <code>@pytest.mark.red</code>.</li>
        <li><strong>Green:</strong> Implement the feature until the test passes.</li>
        <li><strong>Refactor:</strong> Move the test to <code>tests/unit/</code> or <code>tests/integration/</code>.</li>
    </ol>

    <h3>Running Locally (Mac Mini or NUC-2)</h3>
    <p>For local testing without Slack integration:</p>
    <pre><code>./run_agent.sh --local --debug</code></pre>
    <p>This mocks Slack events and lets you test the full request flow without restarting the service.</p>
    <p><strong>For remote testing on NUC-2:</strong></p>
    <pre><code>ssh nuc-2.local
cd /home/earchibald/agents
./run_agent.sh --test
journalctl -u brain-slack-bot -f  # Watch live logs</code></pre>

    <h2>4. Working with Agents ("AI Building AI")</h2>
    
    <h3>Agent Tools</h3>
    <p>The repository includes specialized agents to help <em>you</em> code:</p>
    <ul>
        <li><strong>Advice Agent:</strong> <code>agents/advice_agent.py</code> - Consults on architectural decisions.</li>
        <li><strong>Journal Agent:</strong> <code>agents/journal_agent.py</code> - Summarizes daily work logs.</li>
    </ul>

    <h3>Prompt Engineering Strategy</h3>
    <p>When asking the Copilot to modify the bot:</p>
    <ol>
        <li><strong>Context First:</strong> "Read `slack_agent.py` and `tools/base_tool.py`."</li>
        <li><strong>Clear Intent:</strong> "Create a `WeatherTool` that checks NUC-1 temperature."</li>
        <li><strong>Schema Definition:</strong> "Ensure the tool schema matches the `get_tool_spec()` format."</li>
    </ol>

    <h2>5. Extension Guide</h2>

    <h3>Adding a New Tool</h3>
    <p>Extending the bot's capabilities is straightforward:</p>
    <ol>
        <li>Inherit from <code>BaseTool</code> in <code>slack_bot/tools/</code>.</li>
        <li>Implement <code>execute()</code> and <code>to_tool_spec()</code>.</li>
        <li>Register it in <code>slack_bot/tools/tool_registry.py</code>.</li>
        <li>(Optional) Add a unit test in <code>tests/unit/test_tools.py</code>.</li>
    </ol>

    <h3>Adding a Fact Category</h3>
    <p>To teach the bot new kinds of memory:</p>
    <ul>
        <li>Update <code>FactsStore</code> in <code>slack_bot/tools/builtin/facts_tool.py</code>.</li>
        <li>Update the UI definitions in <code>slack_bot/facts_ui.py</code>.</li>
    </ul>

    <h3>Model Persistence Architecture</h3>
    <p><strong>Storage:</strong> The bot uses <code>ModelPreferenceStore</code> to persist per-user model selections across restarts.</p>
    <ul>
        <li><strong>Storage File:</strong> <code>~/.brain-model-prefs.json</code> on NUC-2 (0600 permissions).</li>
        <li><strong>Format:</strong> <code>{"user_id": {"provider_id": "gemini", "model_name": "gemini-1.5-flash"}}</code></li>
        <li><strong>Load Mechanism:</strong> Preferences are loaded in <code>_generate_with_provider()</code> before each LLM generation.</li>
        <li><strong>Save Mechanism:</strong> Preferences are saved in <code>handle_model_selection()</code> when user changes models via <code>/model</code>.</li>
        <li><strong>Graceful Degradation:</strong> If stored provider/model is unavailable, falls back to default Ollama.</li>
    </ul>
    <p><strong>Implementation Pattern:</strong> ModelPreferenceStore mirrors ApiKeyStore for consistency‚Äîboth use secure JSON storage with 0600 file permissions.</p>

    <h2>6. Deployment Pipeline</h2>
    
    <div class="img-container">
        <img src="report_images/deployment_pipeline.png" alt="Deployment Pipeline Diagram">
    </div>

    <h3>The Deployment Script</h3>
    <p>The <code>deploy_slack_bot.sh</code> script handles the safe rollout to NUC-2.</p>
    <ul>
        <li><strong>Safety Check:</strong> It uses `rsync --exclude` to protect `venv/` and secrets.</li>
        <li><strong>Restart:</strong> Automatically restarts `systemd`.</li>
        <li><strong>Verify:</strong> Checks service status immediately after deployment.</li>
    </ul>

    <h3>Observability</h3>
    <p>Monitor the live bot:</p>
    <pre><code># On NUC-2
journalctl -u brain-slack-bot -f -n 100</code></pre>

    <footer style="margin-top: 4rem; padding-top: 2rem; border-top: 1px solid #eee; color: #666; font-size: 0.9rem;">
        Generated on February 17, 2026 for the Brain Assistant Project.
    </footer>

</body>
</html>
